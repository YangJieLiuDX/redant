L0 范数：向量中非 0 元素的个数。如果用 L0 范数来规则化一个参数矩阵 W 的话，就是希望 W 的大部分元素是 0，即让参数 W 是稀疏的。

例如判断某种病的患病率时，最初有 1000 个特征，建模后参数经过稀疏化，最终只有 5 个特征的参数是非零的，那么就可以说影响患病率的主要就是这 5 个特征。

L1 范数：向量中各元素绝对值之和，也叫稀疏规则算子（Lasso regularization）。任何的规则化算子，如果在 W=0 不可微，且可分解为求和形式，那么这个规则化算子就可以实现稀疏。W 的 L1 范数是绝对值，在 W=0 处不可微。既然 L0 可以实现稀疏，为什么不用 L0，而要用 L1 呢？一是因为 L0 范数很难优化求解（NP 难问题），二是 L1 范数是 L0 范数的最优凸近似，而且它比 L0 范数要容易优化求解。

参数稀疏化的应用：

1. 特征选择：实现特征的自动选择，去除无用特征。稀疏化可以去掉这些无用特征，将特征对应的权重置为零。L1 范式的自由度实际上就是非零参数的个数。
2. 可解释性：如判断某种病的患病率时，最初有 1000 个特征，建模后参数经过稀疏化后只有 5 个特征的参数是非零的，就可以说影响患病率的主要是这 5 个特征。

L2 范数：向量中各元素平方和的平方根，可叫岭回归（Ridge Regression），也叫权值衰减（weight decay），主要用于解决过拟合问题。让 L2 范数的规则项最小，可以使得 W 的每个元素都很小，都接近于 0，但与 L1 范数不同，它不会让它等于 0，而是接近于 0。

L1 正则化给出的最优解 w 是使解更加靠近某些轴，而其它的轴则为 0，所以 L1 正则化能使得到的参数稀疏化。

为什么 L2 正则化比 L1 正则化应用更加广泛？因为 L2 正则化的约束边界光滑且可导，便于采用梯度下降法，而 L1 正则化不可导，只能采用坐标轴下降法或最小角回归法，计算量大。

正则化与贝叶斯先验本质上是一致的，如 L1 正则化与拉普拉斯先验是一致的，L2 正则化与高斯先验是一致的。

Laplace 分布
$$
f(x|\mu,b)=\frac{1}{2b}exp(-\frac{b}{|x-\mu|})
$$


